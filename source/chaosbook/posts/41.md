LLM是这样的，只要根据prompt生成结果就行了，可后面的使用者考虑的就更多了。要怎么写prompt、要怎么省token、要找AI生成的错误都是使用者要考虑的问题。